# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data: brats
  - override /model: mednextv1_small
  - override /callbacks: default
  - override /trainer: default
  - override /logger: wandb.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["brats", "mednext", "SSA"]

seed: 12345

trainer:
  min_epochs: 1
  max_epochs: 3
  gradient_clip_val: 0.5
  accelerator: gpu
  # devices: [0]
  log_every_n_steps: 1
  # logger: True

model:
  net:
    # mednextv1_small
    _target_: src.models.components.mednext.MedNeXt.MedNeXt # points to the MedNeXt class in MedNeXt.py file, which is under mednext folder
    in_channels: 4
    n_classes: 4
    n_channels: 32
    exp_r: 2
    kernel_size: 3
    deep_supervision: False
    do_res: True
    do_res_up_down: True
    block_counts: [2,2,2,2,2,2,2,2,2]


data:
  _target_: src.data.brats_datamodule.BratsDataModule
  # data_dir: ${paths.data_dir}
  data_dir: "C:\\Users\\lenovo\\BraTS2023_SSA_modified_structure\\utilities\\stacked"
  batch_size: 1
  num_workers: 0
  # pin_memory: True

logger:
  wandb:
    project: "BraTS_SSA_MedNeXt_Server"
    tags: ${tags}
    # name: ${experiment_name}
    mode: "offline"
    

callbacks:
  model_checkpoint:
    dirpath: ${paths.output_dir}/checkpoints
    monitor: val_score
    filename: best-checkpoint
    save_top_k: 1
    mode: "max"
    save_last: True

  early_stopping: null